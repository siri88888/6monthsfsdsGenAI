{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab053162-15d3-49c3-94ca-a7a04b0c8839",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ba4372a-1aa3-4e38-857b-034cd506a389",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8957898-3bcd-4f71-83a1-437116ad8ed7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bc6708-6a61-41f0-af9b-6aa161b40dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6405b04f-eeae-4be7-bab6-1ddbb55fc500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f5415d35-4042-466e-a54f-9cf235c586a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96718362-9645-4637-a349-693eed8380fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4e90a726-9eeb-4cf5-a2bd-41f8a17ec730",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorporate\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\data.py:556\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;66;03m# Identify the package (i.e. the .zip file) to download.\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m resource_zipname \u001b[38;5;241m=\u001b[39m resource_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resource_zipname\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    558\u001b[0m     resource_zipname \u001b[38;5;241m=\u001b[39m resource_zipname\u001b[38;5;241m.\u001b[39mrpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find('corporate')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0d94e25-50d6-470f-b360-5958498ae16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI=AI = '''Artificial Intelligence refers to the intelligence of machines. This is in contrast to the natural intelligence of\n",
    "humans and animals. With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and\n",
    "problem-solving. Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines.\n",
    "It is probably the fastest-growing development in the World of technology and innovation. Furthermore, many experts believe\n",
    "AI could solve major challenges and crisis situations.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "830e14a7-2cea-4bf7-9237-1405d856e5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence refers to the intelligence of machines. This is in contrast to the natural intelligence of\\nhumans and animals. With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and\\nproblem-solving. Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines.\\nIt is probably the fastest-growing development in the World of technology and innovation. Furthermore, many experts believe\\nAI could solve major challenges and crisis situations.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7d6d53-f727-44e2-82bb-a30e6a3806ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63776308-ab48-4f6f-88d6-48c356d1b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "type("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abe2e393-7630-4fa7-ac18-7ecbbb1dbbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5438f833-67b1-4dc0-a8b1-933a7da79e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'Intelligence',\n",
       " 'refers',\n",
       " 'to',\n",
       " 'the',\n",
       " 'intelligence',\n",
       " 'of',\n",
       " 'machines',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'in',\n",
       " 'contrast',\n",
       " 'to',\n",
       " 'the',\n",
       " 'natural',\n",
       " 'intelligence',\n",
       " 'of',\n",
       " 'humans',\n",
       " 'and',\n",
       " 'animals',\n",
       " '.',\n",
       " 'With',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " ',',\n",
       " 'machines',\n",
       " 'perform',\n",
       " 'functions',\n",
       " 'such',\n",
       " 'as',\n",
       " 'learning',\n",
       " ',',\n",
       " 'planning',\n",
       " ',',\n",
       " 'reasoning',\n",
       " 'and',\n",
       " 'problem-solving',\n",
       " '.',\n",
       " 'Most',\n",
       " 'noteworthy',\n",
       " ',',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " 'is',\n",
       " 'the',\n",
       " 'simulation',\n",
       " 'of',\n",
       " 'human',\n",
       " 'intelligence',\n",
       " 'by',\n",
       " 'machines',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'probably',\n",
       " 'the',\n",
       " 'fastest-growing',\n",
       " 'development',\n",
       " 'in',\n",
       " 'the',\n",
       " 'World',\n",
       " 'of',\n",
       " 'technology',\n",
       " 'and',\n",
       " 'innovation',\n",
       " '.',\n",
       " 'Furthermore',\n",
       " ',',\n",
       " 'many',\n",
       " 'experts',\n",
       " 'believe',\n",
       " 'AI',\n",
       " 'could',\n",
       " 'solve',\n",
       " 'major',\n",
       " 'challenges',\n",
       " 'and',\n",
       " 'crisis',\n",
       " 'situations',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_tokens=word_tokenize(AI)\n",
    "AI_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d28e6bea-d296-4408-8392-9b37e4c67e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdbe22d5-ec94-4362-bc9a-20a1bd01adc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence refers to the intelligence of machines. This is in contrast to the natural intelligence of\\nhumans and animals. With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and\\nproblem-solving. Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines.\\nIt is probably the fastest-growing development in the World of technology and innovation. Furthermore, many experts believe\\nAI could solve major challenges and crisis situations.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3b10463-fe1a-49a3-b7a1-f08c785f6168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "006f7b73-828a-44a5-8354-d9f50a7da505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial Intelligence refers to the intelligence of machines.',\n",
       " 'This is in contrast to the natural intelligence of\\nhumans and animals.',\n",
       " 'With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and\\nproblem-solving.',\n",
       " 'Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines.',\n",
       " 'It is probably the fastest-growing development in the World of technology and innovation.',\n",
       " 'Furthermore, many experts believe\\nAI could solve major challenges and crisis situations.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_sent=sent_tokenize(AI)\n",
    "AI_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4bee04a-9567-4102-86e9-e7707b89816b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial Intelligence refers to the intelligence of machines. This is in contrast to the natural intelligence of\\nhumans and animals. With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and\\nproblem-solving. Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines.\\nIt is probably the fastest-growing development in the World of technology and innovation. Furthermore, many experts believe\\nAI could solve major challenges and crisis situations.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import blankline_tokenize\n",
    "AI_blank=blankline_tokenize(AI)\n",
    "AI_blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ed62311-433a-4843-844d-a6794ddc2823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab5a6745-763f-4a03-b694-f823a9f00092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial Intelligence refers to the intelligence of machines. This is in contrast to the natural intelligence of\\nhumans and animals. With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and\\nproblem-solving. Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines.\\nIt is probably the fastest-growing development in the World of technology and innovation. Furthermore, many experts believe\\nAI could solve major challenges and crisis situations.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhiotespaceTokenizer\n",
    "AI_blank= WhiotespaceTokenizer().tokenize(AI)\n",
    "AI_blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc5940c7-e13b-485b-a91c-3df5254d03b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'Intelligence',\n",
       " 'refers',\n",
       " 'to',\n",
       " 'the',\n",
       " 'intelligence',\n",
       " 'of',\n",
       " 'machines.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'in',\n",
       " 'contrast',\n",
       " 'to',\n",
       " 'the',\n",
       " 'natural',\n",
       " 'intelligence',\n",
       " 'of',\n",
       " 'humans',\n",
       " 'and',\n",
       " 'animals.',\n",
       " 'With',\n",
       " 'Artificial',\n",
       " 'Intelligence,',\n",
       " 'machines',\n",
       " 'perform',\n",
       " 'functions',\n",
       " 'such',\n",
       " 'as',\n",
       " 'learning,',\n",
       " 'planning,',\n",
       " 'reasoning',\n",
       " 'and',\n",
       " 'problem-solving.',\n",
       " 'Most',\n",
       " 'noteworthy,',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " 'is',\n",
       " 'the',\n",
       " 'simulation',\n",
       " 'of',\n",
       " 'human',\n",
       " 'intelligence',\n",
       " 'by',\n",
       " 'machines.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'probably',\n",
       " 'the',\n",
       " 'fastest-growing',\n",
       " 'development',\n",
       " 'in',\n",
       " 'the',\n",
       " 'World',\n",
       " 'of',\n",
       " 'technology',\n",
       " 'and',\n",
       " 'innovation.',\n",
       " 'Furthermore,',\n",
       " 'many',\n",
       " 'experts',\n",
       " 'believe',\n",
       " 'AI',\n",
       " 'could',\n",
       " 'solve',\n",
       " 'major',\n",
       " 'challenges',\n",
       " 'and',\n",
       " 'crisis',\n",
       " 'situations.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhiotespaceTokenizer\n",
    "AI_blank= WhiotespaceTokenizer().tokenize(AI)\n",
    "AI_blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ce9edeb-4246-420a-8063-0e48df16ebab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "573872d3-c462-42fa-9f73-164e2ad610ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good apple cost $3.88 in hyderabad.please buy two of them.Thanks.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s='Good apple cost $3.88 in hyderabad.please buy two of them.Thanks.'\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2eb9016e-a69b-4ce7-a3f5-b29094ee425b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'apple',\n",
       " 'cost',\n",
       " '$',\n",
       " '3',\n",
       " '.',\n",
       " '88',\n",
       " 'in',\n",
       " 'hyderabad',\n",
       " '.',\n",
       " 'please',\n",
       " 'buy',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f58dba2e-3bf4-40a5-8e48-e266607c8870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'Intelligence',\n",
       " 'refers',\n",
       " 'to',\n",
       " 'the',\n",
       " 'intelligence',\n",
       " 'of',\n",
       " 'machines',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'in',\n",
       " 'contrast',\n",
       " 'to',\n",
       " 'the',\n",
       " 'natural',\n",
       " 'intelligence',\n",
       " 'of',\n",
       " 'humans',\n",
       " 'and',\n",
       " 'animals',\n",
       " '.',\n",
       " 'With',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " ',',\n",
       " 'machines',\n",
       " 'perform',\n",
       " 'functions',\n",
       " 'such',\n",
       " 'as',\n",
       " 'learning',\n",
       " ',',\n",
       " 'planning',\n",
       " ',',\n",
       " 'reasoning',\n",
       " 'and',\n",
       " 'problem',\n",
       " '-',\n",
       " 'solving',\n",
       " '.',\n",
       " 'Most',\n",
       " 'noteworthy',\n",
       " ',',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " 'is',\n",
       " 'the',\n",
       " 'simulation',\n",
       " 'of',\n",
       " 'human',\n",
       " 'intelligence',\n",
       " 'by',\n",
       " 'machines',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'probably',\n",
       " 'the',\n",
       " 'fastest',\n",
       " '-',\n",
       " 'growing',\n",
       " 'development',\n",
       " 'in',\n",
       " 'the',\n",
       " 'World',\n",
       " 'of',\n",
       " 'technology',\n",
       " 'and',\n",
       " 'innovation',\n",
       " '.',\n",
       " 'Furthermore',\n",
       " ',',\n",
       " 'many',\n",
       " 'experts',\n",
       " 'believe',\n",
       " 'AI',\n",
       " 'could',\n",
       " 'solve',\n",
       " 'major',\n",
       " 'challenges',\n",
       " 'and',\n",
       " 'crisis',\n",
       " 'situations',\n",
       " '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_p=wordpunct_tokenize(AI)\n",
    "w_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "600d8ad5-2b7f-438c-b64a-bbd11af7065a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78ac2e89-8a7b-4041-a818-2e1bc05614c3",
   "metadata": {},
   "outputs": [],
   "source": [
    " import nltk\n",
    "from nltk.util  import bigrams,trigrams,ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5af47370-edd1-4268-8d4a-e2b381ddfa55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'world',\n",
       " 'i',\n",
       " 'am',\n",
       " 'datascience',\n",
       " 'student',\n",
       " 'prakash',\n",
       " 'senapathi',\n",
       " 'sir',\n",
       " 'batch']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string=\"hello world i am datascience student prakash senapathi sir batch\"\n",
    "string\n",
    "quotes_tokens=nltk.word_tokenize(string)\n",
    "quotes_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a06cc4a-ef70-45b8-94f0-4d20fb8fc8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 'world'),\n",
       " ('world', 'i'),\n",
       " ('i', 'am'),\n",
       " ('am', 'datascience'),\n",
       " ('datascience', 'student'),\n",
       " ('student', 'prakash'),\n",
       " ('prakash', 'senapathi'),\n",
       " ('senapathi', 'sir'),\n",
       " ('sir', 'batch')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_bigrams=list(nltk.bigrams(quotes_tokens))\n",
    "quotes_bigrams                  \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49f7c2ef-792b-4735-a402-ac89ce999825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 'world', 'i'),\n",
       " ('world', 'i', 'am'),\n",
       " ('i', 'am', 'datascience'),\n",
       " ('am', 'datascience', 'student'),\n",
       " ('datascience', 'student', 'prakash'),\n",
       " ('student', 'prakash', 'senapathi'),\n",
       " ('prakash', 'senapathi', 'sir'),\n",
       " ('senapathi', 'sir', 'batch')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_trigrams=list(nltk.trigrams(quotes_tokens))\n",
    "quotes_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5384b6d9-0490-4ea2-a09a-015246d678ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 'world', 'i', 'am'),\n",
       " ('world', 'i', 'am', 'datascience'),\n",
       " ('i', 'am', 'datascience', 'student'),\n",
       " ('am', 'datascience', 'student', 'prakash'),\n",
       " ('datascience', 'student', 'prakash', 'senapathi'),\n",
       " ('student', 'prakash', 'senapathi', 'sir'),\n",
       " ('prakash', 'senapathi', 'sir', 'batch')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_ngrams=list(nltk.ngrams(quotes_tokens,4))\n",
    "quotes_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "581847aa-c274-4b2e-bf38-2b4e91b4094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem  import PorterStemmer\n",
    "pst=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4ad24f1-2d7c-437c-9d46-92d09c8db950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'affect'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('affection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5413dc8-02a7-419f-8a64-e36ec35dba32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'play'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('playing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5974a88-a770-4adf-95a2-5b6c51c7e6d4",
   "metadata": {},
   "source": [
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca4b2e66-0c38-48c9-86e2-ece306c8d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_stem=['give','giving','given','gave'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee60a5a5-e71b-431b-b75d-b9bf203d0650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:give\n",
      "given:given\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "for words in word_to_stem:\n",
    "    print(words+':' +pst.stem(words))\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4fd8ad6-2fbf-447a-bdb5-86cf9558a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem  import LancasterStemmer\n",
    "lst=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e55b78f1-3881-438f-b150-4c815ede5d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:give\n",
      "given:given\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    " for words in word_to_stem:\n",
    "      print(words+':' +pst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d05c120e-0db3-4e9b-a108-c6051d8c2e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem  import SnowballStemmer\n",
    "sbst=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f5bcc33-5843-4637-9beb-0eaf4c01080a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:give\n",
      "given:given\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "for words in word_to_stem:\n",
    "      print(words+':' +sbst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bd61bcc-85e9-478b-b9bc-ef771b0412b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autobahn'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer=SnowballStemmer('german')\n",
    "stemmer.stem('Autobahnen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "31b70bfc-b6c8-420f-ae22-ba9ee0ef8d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ttwrd\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9d84b0d8-d78e-4a46-930d-68783668f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem  import wordnet\n",
    "from nltk.stem  import WordNetLemmatizer\n",
    "word_lem=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a25fc81d-9a5a-4a4c-bb5f-5c7b5a79aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_stem=['give','giving','given','gaved','thinking','loving','maximum','prakash senapthi'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b768c199-2243-4656-aa3e-30822174a77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:giving\n",
      "given:given\n",
      "gaved:gaved\n",
      "thinking:thinking\n",
      "loving:loving\n",
      "maximum:maximum\n",
      "prakash senapthi:prakash senapthi\n"
     ]
    }
   ],
   "source": [
    "for words in word_to_stem:\n",
    "      print(words+':' +word_lem.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "02fcfdb7-236b-4df8-bbf7-6e0cd0caba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "word=\"running\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "27c549c7-73a3-4902-a0a6-8095923dbf8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ttwrd/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ttwrd\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ttwrd/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ttwrd\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m word_lem\u001b[38;5;241m.\u001b[39mlemmatize(word)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\stem\\wordnet.py:85\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` by picking the shortest of the possible lemmas,\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    using the wordnet corpus reader's built-in _morphy function.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    :return: The shortest lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_morphy(word, pos)\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\stem\\wordnet.py:41\u001b[0m, in \u001b[0;36mWordNetLemmatizer._morphy\u001b[1;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m_morphy() is WordNet's _morphy lemmatizer.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03mIt returns a list of all lemmas found in WordNet.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m['us', 'u']\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wordnet \u001b[38;5;28;01mas\u001b[39;00m wn\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wn\u001b[38;5;241m.\u001b[39m_morphy(form, pos, check_exceptions)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:120\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ttwrd/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ttwrd\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "word_lem.lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0d683411-19a8-42d4-b626-45f351b0e97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:giving\n",
      "given:given\n",
      "gaved:gaved\n",
      "thinking:thinking\n",
      "loving:loving\n",
      "maximum:maximum\n",
      "prakash senapthi:prakash senapthi\n"
     ]
    }
   ],
   "source": [
    "for words in word_to_stem:\n",
    "      print(words+':' +word_lem.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f9c880-e9f3-4958-a090-47ac4cd6b6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
